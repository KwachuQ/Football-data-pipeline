# tests/test_statistics_extractor.py
#!/usr/bin/env python3
"""
Safe Testing Suite for statistics_extractor.py
Uses isolated test bucket and mock API responses
"""

import json
import logging
import os
import sys
from datetime import datetime
from typing import Dict, Any
from unittest.mock import MagicMock, patch, Mock
from io import BytesIO

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'etl', 'bronze', 'extractors'))

from minio import Minio

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Test Configuration
# ============================================================================

TEST_BUCKET = "bronze-test"  # Isolated test bucket
TEST_RAPIDAPI_KEY = "test_key_12345"
TEST_MATCH_IDS = [9000001, 9000002, 9000003]

# ============================================================================
# Mock Response Generator
# ============================================================================

class MockResponse:
    """Mock requests.Response object"""
    
    def __init__(self, status_code: int, json_data: Dict[str, Any] = None, text: str = ""):
        self.status_code = status_code
        self._json_data = json_data or {}
        self.text = text
    
    def json(self):
        return self._json_data

def generate_mock_statistics(match_id: int) -> Dict[str, Any]:
    """Generate realistic mock statistics data"""
    return {
        "statistics": [
            {
                "period": "ALL",
                "groups": [
                    {
                        "groupName": "TVData",
                        "statisticsItems": [
                            {
                                "name": "Ball possession",
                                "home": "55%",
                                "away": "45%",
                                "compareCode": 1
                            },
                            {
                                "name": "Total shots",
                                "home": "15",
                                "away": "12",
                                "compareCode": 1
                            },
                            {
                                "name": "Shots on target",
                                "home": "7",
                                "away": "5",
                                "compareCode": 1
                            }
                        ]
                    },
                    {
                        "groupName": "Passes",
                        "statisticsItems": [
                            {
                                "name": "Total passes",
                                "home": "445",
                                "away": "378",
                                "compareCode": 1
                            },
                            {
                                "name": "Accurate passes",
                                "home": "387",
                                "away": "321",
                                "compareCode": 1
                            }
                        ]
                    }
                ]
            }
        ]
    }

def mock_requests_get(url, headers=None, params=None, timeout=None):
    """Mock requests.get with various responses"""
    match_id = int(params.get('matchId', 0))
    
    # Simulate different scenarios
    if match_id == 9000001:
        # Success case
        return MockResponse(200, generate_mock_statistics(match_id))
    
    elif match_id == 9000002:
        # Not found case
        return MockResponse(404, text="Match not found")
    
    elif match_id == 9000003:
        # Error case
        return MockResponse(500, text="Internal server error")
    
    else:
        # Default success
        return MockResponse(200, generate_mock_statistics(match_id))

# ============================================================================
# Test Statistics Fetcher
# ============================================================================

class TestStatisticsFetcher:
    """Statistics fetcher with test-specific configuration"""
    
    def __init__(self):
        """Initialize with test bucket"""
        from statistics_extractor import StatisticsFetcher
        
        # Create fetcher with test API key
        self.fetcher = StatisticsFetcher(
            rapidapi_key=TEST_RAPIDAPI_KEY,
            tournament_id=999,
            season_folder="test_season"
        )
        
        # Override MinIO client to use test bucket
        self.fetcher.minio_client = Minio(
            endpoint=os.getenv('MINIO_ENDPOINT', 'localhost:9000'),
            access_key=os.getenv('MINIO_ACCESS_KEY', 'minio'),
            secret_key=os.getenv('MINIO_SECRET_KEY', 'minio123'),
            secure=os.getenv('MINIO_SECURE', 'false').lower() == 'true'
        )
        self.fetcher.bucket_name = TEST_BUCKET
        
        # Ensure test bucket exists
        self._ensure_test_bucket()
        
        logger.info(f"üß™ Test StatisticsFetcher initialized with bucket: {TEST_BUCKET}")
    
    def _ensure_test_bucket(self):
        """Create test bucket if it doesn't exist"""
        try:
            if not self.fetcher.minio_client.bucket_exists(TEST_BUCKET):
                self.fetcher.minio_client.make_bucket(TEST_BUCKET)
                logger.info(f"‚úì Created test bucket: {TEST_BUCKET}")
        except Exception as e:
            logger.error(f"Error creating test bucket: {e}")
            raise

# ============================================================================
# Test Functions
# ============================================================================

def test_initialization():
    """Test fetcher initialization"""
    logger.info("\n" + "="*60)
    logger.info("TEST 1: Initialization")
    logger.info("="*60)
    
    try:
        test_fetcher = TestStatisticsFetcher()
        
        assert test_fetcher.fetcher.tournament_id == 999
        assert test_fetcher.fetcher.bucket_name == TEST_BUCKET
        assert test_fetcher.fetcher.rapidapi_key == TEST_RAPIDAPI_KEY
        
        logger.info("‚úì API key set correctly")
        logger.info("‚úì Tournament ID set correctly")
        logger.info("‚úì Test bucket configured")
        logger.info("\n‚úÖ Initialization test passed!")
        
        return test_fetcher
        
    except Exception as e:
        logger.error(f"‚ùå Initialization test failed: {e}")
        raise

def test_object_name_generation():
    """Test MinIO object path generation"""
    logger.info("\n" + "="*60)
    logger.info("TEST 2: Object Name Generation")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # Test with explicit date
    object_name = test_fetcher.fetcher._get_object_name(12345, "2025-11-13")
    expected = "match_statistics/test_season/date=2025-11-13/match_12345.json"
    
    assert object_name == expected, f"Expected {expected}, got {object_name}"
    logger.info(f"‚úì Object path: {object_name}")
    
    # Test with default date
    object_name_default = test_fetcher.fetcher._get_object_name(12345)
    logger.info(f"‚úì Default date path: {object_name_default}")
    
    logger.info("\n‚úÖ Object name generation test passed!")

def test_fetch_success():
    """Test successful statistics fetch"""
    logger.info("\n" + "="*60)
    logger.info("TEST 3: Fetch Statistics - Success")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # Mock requests.get
    with patch('requests.get', side_effect=mock_requests_get):
        result = test_fetcher.fetcher.fetch_match_statistics(9000001)
    
    logger.info(f"üìä Result:")
    logger.info(f"  ‚Ä¢ Match ID: {result['match_id']}")
    logger.info(f"  ‚Ä¢ Status: {result['status']}")
    logger.info(f"  ‚Ä¢ Has data: {result['data'] is not None}")
    
    assert result['status'] == 'success'
    assert result['match_id'] == 9000001
    assert result['data'] is not None
    assert 'statistics' in result['data']
    
    logger.info("\n‚úÖ Successful fetch test passed!")

def test_fetch_not_found():
    """Test fetch when match not found"""
    logger.info("\n" + "="*60)
    logger.info("TEST 4: Fetch Statistics - Not Found")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    with patch('requests.get', side_effect=mock_requests_get):
        result = test_fetcher.fetcher.fetch_match_statistics(9000002)
    
    logger.info(f"üìä Result:")
    logger.info(f"  ‚Ä¢ Match ID: {result['match_id']}")
    logger.info(f"  ‚Ä¢ Status: {result['status']}")
    
    assert result['status'] == 'not_found'
    assert result['match_id'] == 9000002
    assert result['data'] is None
    
    logger.info("\n‚úÖ Not found test passed!")

def test_fetch_error():
    """Test fetch with API error"""
    logger.info("\n" + "="*60)
    logger.info("TEST 5: Fetch Statistics - Error")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    with patch('requests.get', side_effect=mock_requests_get):
        result = test_fetcher.fetcher.fetch_match_statistics(9000003)
    
    logger.info(f"üìä Result:")
    logger.info(f"  ‚Ä¢ Match ID: {result['match_id']}")
    logger.info(f"  ‚Ä¢ Status: {result['status']}")
    logger.info(f"  ‚Ä¢ Error: {result.get('error', 'N/A')[:50]}")
    
    assert result['status'] == 'error'
    assert result['match_id'] == 9000003
    assert 'error' in result
    
    logger.info("\n‚úÖ Error handling test passed!")

def test_save_to_minio():
    """Test saving statistics to MinIO"""
    logger.info("\n" + "="*60)
    logger.info("TEST 6: Save to MinIO")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # Generate test data
    test_data = generate_mock_statistics(9000001)
    
    # Save to MinIO
    success, object_path = test_fetcher.fetcher.save_to_minio(
        match_id=9000001,
        data=test_data,
        match_date="2025-11-13"
    )
    
    logger.info(f"üìä Result:")
    logger.info(f"  ‚Ä¢ Success: {success}")
    logger.info(f"  ‚Ä¢ Path: {object_path}")
    
    assert success, "Save should succeed"
    assert object_path, "Should return object path"
    
    # Verify object exists
    try:
        obj = test_fetcher.fetcher.minio_client.stat_object(TEST_BUCKET, object_path)
        logger.info(f"‚úì Object created: {obj.object_name} ({obj.size} bytes)")
        
        # Download and verify content
        response = test_fetcher.fetcher.minio_client.get_object(TEST_BUCKET, object_path)
        content = json.loads(response.read().decode('utf-8'))
        
        assert 'match_id' in content
        assert 'statistics' in content
        assert 'metadata' in content
        assert content['match_id'] == 9000001
        
        logger.info(f"‚úì Content verified:")
        logger.info(f"  ‚Ä¢ Match ID: {content['match_id']}")
        logger.info(f"  ‚Ä¢ Has statistics: {'statistics' in content}")
        logger.info(f"  ‚Ä¢ Has metadata: {'metadata' in content}")
        logger.info(f"  ‚Ä¢ Ingestion time: {content['metadata'].get('ingestion_timestamp', 'N/A')[:19]}")
        
    except Exception as e:
        logger.error(f"‚ùå Verification failed: {e}")
        raise
    
    logger.info("\n‚úÖ Save to MinIO test passed!")

def test_duplicate_detection():
    """Test duplicate detection"""
    logger.info("\n" + "="*60)
    logger.info("TEST 7: Duplicate Detection")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # Save first time
    test_data = generate_mock_statistics(9000010)
    success1, path1 = test_fetcher.fetcher.save_to_minio(
        match_id=9000010,
        data=test_data,
        match_date="2025-11-13"
    )
    
    logger.info(f"‚úì First save: {success1}")
    
    # Check if exists
    exists = test_fetcher.fetcher.statistics_exist(9000010, "2025-11-13")
    logger.info(f"‚úì Statistics exist: {exists}")
    
    assert exists, "Statistics should exist after save"
    
    # Check non-existent match
    not_exists = test_fetcher.fetcher.statistics_exist(9999999, "2025-11-13")
    logger.info(f"‚úì Non-existent match: {not_exists}")
    
    assert not not_exists, "Non-existent statistics should return False"
    
    logger.info("\n‚úÖ Duplicate detection test passed!")

def test_batch_processing():
    """Test batch processing with mixed results"""
    logger.info("\n" + "="*60)
    logger.info("TEST 8: Batch Processing")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # Test batch with mixed results
    test_match_ids = [9000001, 9000002, 9000003]  # success, not_found, error
    
    with patch('requests.get', side_effect=mock_requests_get):
        result = test_fetcher.fetcher.fetch_and_save_statistics(
            match_ids=test_match_ids,
            delay_between_requests=0.1,  # Fast for testing
            skip_existing=False
        )
    
    logger.info(f"\nüìä Batch Results:")
    logger.info(f"  ‚Ä¢ Total: {result['total']}")
    logger.info(f"  ‚Ä¢ Successful: {result['successful']}")
    logger.info(f"  ‚Ä¢ Failed: {result['failed']}")
    logger.info(f"  ‚Ä¢ Skipped: {result['skipped']}")
    
    logger.info(f"\nüìã Details:")
    for detail in result['details']:
        logger.info(f"  ‚Ä¢ Match {detail['match_id']}: {detail['status']}")
    
    assert result['total'] == 3
    assert result['successful'] == 1  # Only 9000001 succeeds
    assert result['skipped'] == 1  # 9000002 not found
    assert result['failed'] == 1  # 9000003 errors
    
    logger.info("\n‚úÖ Batch processing test passed!")

def test_skip_existing():
    """Test skipping existing statistics"""
    logger.info("\n" + "="*60)
    logger.info("TEST 9: Skip Existing Statistics")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # First run - save statistics
    with patch('requests.get', side_effect=mock_requests_get):
        result1 = test_fetcher.fetcher.fetch_and_save_statistics(
            match_ids=[9000001],
            skip_existing=False
        )
    
    logger.info(f"‚úì First run: {result1['successful']} successful")
    
    # Second run - should skip
    with patch('requests.get', side_effect=mock_requests_get) as mock_get:
        result2 = test_fetcher.fetcher.fetch_and_save_statistics(
            match_ids=[9000001],
            skip_existing=True
        )
        
        # Verify API was not called
        assert mock_get.call_count == 0, "API should not be called for existing stats"
    
    logger.info(f"‚úì Second run: {result2['skipped']} skipped")
    logger.info(f"‚úì API calls: 0 (correctly skipped)")
    
    assert result2['skipped'] == 1
    assert result2['successful'] == 0
    
    logger.info("\n‚úÖ Skip existing test passed!")

def verify_data_isolation():
    """Verify test data is isolated"""
    logger.info("\n" + "="*60)
    logger.info("TEST 10: Data Isolation Verification")
    logger.info("="*60)
    
    test_fetcher = TestStatisticsFetcher()
    
    # List objects in test bucket
    try:
        objects = list(test_fetcher.fetcher.minio_client.list_objects(
            TEST_BUCKET, recursive=True
        ))
        logger.info(f"\nüì¶ Objects in {TEST_BUCKET}: {len(objects)}")
        
        for obj in objects[:10]:
            logger.info(f"  ‚Ä¢ {obj.object_name} ({obj.size} bytes)")
        
        # Check production bucket unchanged
        try:
            prod_objects = list(test_fetcher.fetcher.minio_client.list_objects(
                "bronze", recursive=True
            ))
            logger.info(f"\n‚úÖ Production bucket 'bronze' has {len(prod_objects)} objects (unchanged)")
        except Exception:
            logger.info(f"\n‚úÖ Production bucket 'bronze' doesn't exist or is empty (safe)")
        
    except Exception as e:
        logger.error(f"‚ùå Error checking buckets: {e}")
        raise
    
    logger.info("\n‚úÖ Data isolation verified!")

# ============================================================================
# Cleanup Utilities
# ============================================================================

def cleanup_test_data():
    """Remove all test data"""
    logger.info("\n" + "="*60)
    logger.info("CLEANUP: Removing Test Data")
    logger.info("="*60)
    
    minio_client = Minio(
        endpoint=os.getenv('MINIO_ENDPOINT', 'localhost:9000'),
        access_key=os.getenv('MINIO_ACCESS_KEY', 'minio'),
        secret_key=os.getenv('MINIO_SECRET_KEY', 'minio123'),
        secure=os.getenv('MINIO_SECURE', 'false').lower() == 'true'
    )
    
    try:
        objects = list(minio_client.list_objects(TEST_BUCKET, recursive=True))
        logger.info(f"Found {len(objects)} objects to remove")
        
        for obj in objects:
            minio_client.remove_object(TEST_BUCKET, obj.object_name)
            logger.info(f"  ‚úì Removed: {obj.object_name}")
        
        logger.info("\n‚úÖ Cleanup complete!")
        
    except Exception as e:
        logger.error(f"‚ùå Cleanup error: {e}")

# ============================================================================
# Main Test Runner
# ============================================================================

def run_all_tests():
    """Run all tests"""
    logger.info("\n" + "="*70)
    logger.info("üß™ STATISTICS_EXTRACTOR.PY - SAFE TEST SUITE")
    logger.info("="*70)
    logger.info(f"Test bucket: {TEST_BUCKET}")
    logger.info(f"Test API key: {TEST_RAPIDAPI_KEY[:20]}...")
    logger.info(f"Test match IDs: {TEST_MATCH_IDS}")
    logger.info("="*70)
    
    try:
        # Run tests
        test_initialization()
        test_object_name_generation()
        test_fetch_success()
        test_fetch_not_found()
        test_fetch_error()
        test_save_to_minio()
        test_duplicate_detection()
        test_batch_processing()
        test_skip_existing()
        verify_data_isolation()
        
        logger.info("\n" + "="*70)
        logger.info("‚úÖ ALL TESTS PASSED!")
        logger.info("="*70)
        logger.info("\nKey features verified:")
        logger.info("  ‚úì API request mocking works correctly")
        logger.info("  ‚úì Retry logic configured (not tested due to mock)")
        logger.info("  ‚úì Duplicate detection prevents re-fetching")
        logger.info("  ‚úì Batch processing handles mixed results")
        logger.info("  ‚úì Metadata enrichment included")
        logger.info("  ‚úì Production data completely isolated")
        logger.info("  ‚úì Synchronous execution (correct for DAGs)")
        
    except Exception as e:
        logger.error(f"\n‚ùå Test suite failed: {e}", exc_info=True)
        raise
    finally:
        logger.info("\n" + "="*70)
        logger.info("Test data created in bucket: " + TEST_BUCKET)
        logger.info("="*70)


if __name__ == "__main__":
    # Run tests
    run_all_tests()
    
    # Optional: Cleanup
    print("\n" + "="*70)
    response = input("Do you want to cleanup test data? (y/N): ")
    if response.lower() == 'y':
        cleanup_test_data()
    else:
        print("Test data preserved for inspection.")